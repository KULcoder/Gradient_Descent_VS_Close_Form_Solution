{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Dataset and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "Phi = np.array([[0, 2], [1, 0], [0, -2], [-1, 0], [0, 0]])\n",
    "y = np.array([1, 1, 1, 1, -1])\n",
    "n = 5\n",
    "\n",
    "def transform(X):\n",
    "    bias = np.repeat(1, len(X))\n",
    "    dim1 = X[:, 0]**2\n",
    "    dim2 = X[:, 1]**2\n",
    "    dim3 = np.sqrt(2) * X[:, 0]\n",
    "    dim4 = np.sqrt(2) * X[:, 1]\n",
    "    dim5 = np.sqrt(2) * X[:, 0] * X[:, 1]\n",
    "    return np.column_stack([bias, dim1, dim2, dim3, dim4, dim5])\n",
    "\n",
    "new_Phi = transform(Phi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants and Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants\n",
    "lambda_ = 2 # regularization parameter\n",
    "alpha = 1e-4 # learning rate\n",
    "max_iter = 1000 # maximum number of iterations\n",
    "tol = 1e-4 # tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w, gradient, alpha, max_iter, tol):\n",
    "    \"\"\"\n",
    "    A method to perform gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of N\n",
    "        data points; each point has dimension D.\n",
    "    - y: A numpy array of shape (N,) containing labels for the minibatch.\n",
    "    - w: A numpy array of shape (D,) containing weights.\n",
    "    - gradient: A function that computes the gradient of the loss function\n",
    "        with respect to weights w; the function should take the same\n",
    "    - alpha: (float) learning rate for optimization.\n",
    "    - max_iter: (integer) maximum number of iterations to perform.\n",
    "    - tol: (float) tolerance value for stopping.\n",
    "\n",
    "    Returns:\n",
    "    - w: The learned weight vector.\n",
    "    \"\"\"\n",
    "    for _ in range(int(max_iter)):  # set maximum number of iterations\n",
    "        # update weights\n",
    "        w -= alpha * gradient(X, y, w, lambda_)\n",
    "        if np.linalg.norm(gradient(X, y, w, lambda_)) < tol:\n",
    "            # if the norm of the gradient is smaller than the tolerance, perform early stopping\n",
    "            break\n",
    "    return w\n",
    "\n",
    "def ridge_gradient_SSE(X, y, w, lambda_):\n",
    "    \"\"\"\n",
    "    A method to compute the gradient of the SSE Ridge (l2) loss function with respect to weights w.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of N\n",
    "        data points; each point has dimension D.\n",
    "    - y: A numpy array of shape (N,) containing labels for the minibatch.\n",
    "    - w: A numpy array of shape (D,) containing weights.\n",
    "    - lambda_: (float) regularization parameter.\n",
    "\n",
    "    Returns:\n",
    "    - grad: A numpy array of shape (D,) containing the gradient of the loss function with respect to w.\n",
    "    \"\"\"\n",
    "    # compute gradient\n",
    "    grad = 2 * lambda_ * w - 2 * X.T @ (y - X @ w) \n",
    "    return grad\n",
    "\n",
    "def ridge_gradient_MSE(X, y, w, lambda_):\n",
    "    \"\"\"\n",
    "    A method to compute the gradient of the MSE Ridge (l2) loss function with respect to weights w.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of N\n",
    "        data points; each point has dimension D.\n",
    "    - y: A numpy array of shape (N,) containing labels for the minibatch.\n",
    "    - w: A numpy array of shape (D,) containing weights.\n",
    "    - lambda_: (float) regularization parameter.\n",
    "\n",
    "    Returns:\n",
    "    - grad: A numpy array of shape (D,) containing the gradient of the loss function with respect to w.\n",
    "    \"\"\"\n",
    "    # compute gradient\n",
    "    grad = 2 * lambda_ * w - 2 * X.T @ (y - X @ w) / len(X)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_withlog(X, y, w, gradient, alpha, max_iter, tol):\n",
    "    \"\"\"\n",
    "    A method to perform gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of N\n",
    "        data points; each point has dimension D.\n",
    "    - y: A numpy array of shape (N,) containing labels for the minibatch.\n",
    "    - w: A numpy array of shape (D,) containing weights.\n",
    "    - gradient: A function that computes the gradient of the loss function\n",
    "        with respect to weights w; the function should take the same\n",
    "    - alpha: (float) learning rate for optimization.\n",
    "    - max_iter: (integer) maximum number of iterations to perform.\n",
    "    - tol: (float) tolerance value for stopping.\n",
    "\n",
    "    Returns:\n",
    "    - w: The learned weight vector.\n",
    "    - log: A list of weights at each iteration.\n",
    "    \"\"\"\n",
    "    log = []\n",
    "    for i in range(int(max_iter)):  # set maximum number of iterations\n",
    "        # update weights\n",
    "        w -= alpha * gradient(X, y, w, lambda_)\n",
    "        if np.linalg.norm(gradient(X, y, w, lambda_)) < tol:\n",
    "            # if the norm of the gradient is smaller than the tolerance, perform early stopping\n",
    "            break\n",
    "        log.append(w)\n",
    "    return w, log"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the Experiment\n",
    "The only reason is that sklearn use SSE rather than MSE as the loss function. In every gradient, the MSE is divided by the size of the dataset, while the SSE is not.\n",
    "This is normally not a problem, but when we have regularization, the SSE method strength the regularization effect, while the MSE method weaken it (since we divide the loss part).\n",
    "- In other words, $\\lambda = 2$ in MSE is stronger in regularization than $\\lambda = 2$ in SSE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent SSE vs sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03042573,  0.48335373,  0.22810159,  0.        , -0.        ,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GD with SSE\n",
    "w = np.zeros(new_Phi.shape[1])\n",
    "w = gradient_descent(new_Phi, y, w, ridge_gradient_SSE, 0.001, max_iter, tol)\n",
    "w.round(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02857143, 0.48571429, 0.22857143, 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn solution\n",
    "model = Ridge(fit_intercept=False, alpha=lambda_)\n",
    "model.fit(new_Phi, y)\n",
    "model.coef_.round(12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent MSE vs Close Form Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08752787, 0.15083731, 0.17377868, 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GD with MSE\n",
    "w = np.zeros(new_Phi.shape[1])\n",
    "w = gradient_descent(new_Phi, y, w, ridge_gradient_MSE, 0.001, max_iter, tol)\n",
    "w.round(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08695652, 0.15217391, 0.17391304, 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# close form solution\n",
    "shape = new_Phi.T.shape[0]\n",
    "w_close_form = np.linalg.solve( new_Phi.T @ new_Phi + n*lambda_*np.identity(shape), new_Phi.T @ y )\n",
    "w_close_form"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How precise can we get from gradient descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08695667,  0.15217373,  0.17391301, -0.        ,  0.        ,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GD with MSE\n",
    "w = np.zeros(new_Phi.shape[1])\n",
    "w = gradient_descent(new_Phi, y, w, ridge_gradient_MSE, alpha=1e-4, max_iter=1e5, tol=1e-6)\n",
    "w.round(12)\n",
    "# this is pretty close!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we understand the reason, can we use ridge to obtain the same result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08695652, 0.15217391, 0.17391304, 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn solution\n",
    "model = Ridge(fit_intercept=False, alpha=lambda_*len(y)) # greater the regularization!\n",
    "model.fit(new_Phi, y)\n",
    "model.coef_.round(12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96a4002f748d07e64de439cdbf79dfdc2429568c8e6deb2d0c50edf41253abf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
